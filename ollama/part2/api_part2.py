
# from fastapi import FastAPI, HTTPException
# from pydantic import BaseModel
# from fastapi.concurrency import run_in_threadpool
# import chromadb
# from sentence_transformers import SentenceTransformer
# import httpx
# import random
# import json
# from datetime import datetime
# import demjson3 as demjson
# import re
# import uuid
# import os
# from fastapi.middleware.cors import CORSMiddleware

# app = FastAPI(title="TOEIC Part 2 Generator API")
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_methods=["*"],
#     allow_headers=["*"]
# )
# # -----------------------------
# # é…ç½®
# # -----------------------------
# CHROMA_PATH = "/app/chroma_db"
# JSONL_PATH = "/app/json/toeic_part2_db_ready.jsonl"
# OLLAMA_API_URL = "http://ollama:11434/api/generate"

# embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
# chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)

# collection = chroma_client.get_collection("toeic_part2")
# scenario_collection = chroma_client.get_collection("toeic_scenarios")


# # -----------------------------
# # Pydantic è«‹æ±‚æ¨¡å‹
# # -----------------------------
# class GenerateRequest(BaseModel):
#     query_text: str = "Part 2 TOEIC listening conversation with 3 option"

# # -----------------------------
# # å·¥å…·å‡½æ•¸
# # -----------------------------
# def clean_text(text: str):
#     """ç§»é™¤éæ³•å­—å…ƒ"""
#     return re.sub(r'[\x00-\x09\x0b\x0c\x0e-\x1f\x7f]', '', text)




# def generate_uuid() -> str:
#     return str(uuid.uuid4())

# # -----------------------------
# # FastAPI è·¯ç”±
# # -----------------------------
# @app.post("/generate_part2")
# def generate_part2(req: GenerateRequest):
#     # é¸æ“‡æƒ…å¢ƒ
#     scenario_keywords = [
#         "ä¼æ¥­ç™¼å±•", "å¤–é£Ÿ", "å¨›æ¨‚", "é‡‘èï¼é ç®—", "ä¸€èˆ¬å•†å‹™", "ä¿å¥", "æˆ¿å±‹ï¼å…¬å¸åœ°ç”¢",
#         "è£½é€ æ¥­", "è¾¦å…¬å®¤", "äººäº‹", "æ¡è³¼", "æŠ€è¡“å±¤é¢", "æ—…éŠ"
#     ]
#     scenario_text = random.choice(scenario_keywords)

#     # # æŸ¥è©¢èªæ„ç›¸ä¼¼é¡Œç›®
#     # query_embedding = await run_in_threadpool(embedding_model.encode, req.query_text)
#     # results = collection.query(query_embeddings=[query_embedding], n_results=2)
#     # similar_questions = results["documents"][0] if results["documents"] else []

#     # scenario_embedding = await run_in_threadpool(embedding_model.encode, scenario_text)
#     # scenario_results = scenario_collection.query(query_embeddings=[scenario_embedding], n_results=1)
#     # scenario_context = scenario_results["documents"][0][0] if scenario_results["documents"] else scenario_text
# # æŸ¥è©¢ç›¸ä¼¼é¡Œç›®
#     query_embedding = embedding_model.encode(req.query_text).tolist()
#     results = collection.query(query_embeddings=[query_embedding], n_results=3)
#     similar_questions = results["documents"][0]

#     # æŸ¥è©¢æƒ…å¢ƒç¯„ä¾‹
#     scenario_embedding = embedding_model.encode(scenario_text).tolist()
#     scenario_results = scenario_collection.query(query_embeddings=[scenario_embedding], n_results=1)
#     scenario_context = scenario_results["documents"][0][0] if scenario_results["documents"] else scenario_text

#     context = f"Example: {similar_questions[0][:500]}..."

#     # ğŸ”§ prompt
#     prompt = f"""
# ä½ æ˜¯ä¸€ä½å°ˆæ¥­ TOEIC å‡ºé¡Œå°ˆå®¶ã€‚è«‹æ ¹æ“šä¸‹åˆ—æ¸¬é©—æƒ…å¢ƒèˆ‡ç¯„ä¾‹é¡Œç›®ï¼Œç”Ÿæˆä¸€é¡Œæ–°çš„ TOEIC Part 2 è½åŠ›é¡Œçµ„ï¼ŒåŒ…å«ï¼š

# - è¼¸å‡ºjsonæ ¼å¼å¦‚ä¸‹ï¼š
# {{
#   "toeic_listeningmaterial": {{
#     "audio_url": "",
#     "transcript": "Question. é€™è£¡æ”¾å•å¥å…§å®¹ï¼Œæ¥ä¸‰å€‹é¸é …",
#     "accent": "",
#     "topic": "{scenario_text}",
#     "speaker_count": "1",
#     "listening_level": "beginner,intermediate,advanced",
#   }},
#   "toeic_questions": [
#     {{
#       "question_text": "transcriptå•å¥å…§å®¹,
#       "question_type": "listen",
#       "question_category": "ï¼ˆtense / pos / syntax / vocab æ“‡ä¸€",
#       "passage_id": null,
#       "question_image_url": null,
#       "part": "2",
#       "option_a_text": "é¸é … A",
#       "option_b_text": "é¸é … B",
#       "option_c_text": "é¸é … C",
#       "is_correct": "A",
#       "difficulty_level": "1-5",
#       "explanation": "è§£æ",
#     }}
#   ]
# }}

# ### æ¸¬é©—æƒ…å¢ƒï¼š
# {scenario_context}

# ### ç¯„ä¾‹é¡Œç›®ï¼š
# {context}
# è«‹ç‰¹åˆ¥æ³¨æ„ï¼š
# - ç‚ºé¡Œç›®ä¸€å¥å•å¥ï¼Œé¸é …ä¸‰å¥ï¼Œé¸é …å¿…é ˆå‡ºç¾åœ¨transcpritçš„é¡Œç›®å¾Œé¢ï¼Œé¡Œç›®ä»¥å°è©±æˆ–å•ç­”ç‚ºä¸»ï¼Œå«ä¸­æ–‡ explanationã€‚
# - å•†æ¥­è‹±æ–‡è‡ªç„¶å°ˆæ¥­ã€é¸é …èªæ„éœ€æ¥è¿‘ä¸”åˆæ–‡æ³•ã€‚
# - explanation é ˆç”¨ç¹é«”ä¸­æ–‡æŒ‡å‡ºæ–‡æ³•é‡é»æˆ–èªæ„å·®ç•°ï¼Œä¸å¯ç©ºæ³›ã€‚
# è«‹ç›´æ¥**è¼¸å‡ºç´” JSONï¼Œä¸è¦åŠ å…¥æ–‡å­—å‰ç¶´æˆ– Markdown**ã€‚
# """.strip()
#     # 5) å‘¼å«æœ¬åœ° LLMï¼ˆOllamaï¼‰
    
#     # try:
#     #     async with httpx.AsyncClient(timeout=120.0) as client:
#     #         r = await client.post(
#     #             OLLAMA_API_URL,
#     #             json={"model": "llama3", "prompt": prompt, "stream": False}
#     #         )
#     #         r.raise_for_status()
#     #         raw_output = r.json().get("response", "")
#     # except httpx.HTTPError as e:
#     #     raise HTTPException(status_code=502, detail=f"Ollama éŒ¯èª¤: {e}")
#     import requests
#     response = requests.post(
#         OLLAMA_API_URL,
#         json={
#             "model": "llama3",
#             "prompt": prompt,
#             "stream": False
#         }
#     )
#     raw_output = response.json()["response"]
#     cleaned_output = clean_text(raw_output)


#     # å˜—è©¦è§£æ JSON
#     try:
#         data = json.loads(cleaned_output)
#     except json.JSONDecodeError:
#         data = demjson.decode(cleaned_output)

#     # ç”Ÿæˆ UUID
#     material_id = generate_uuid()
#     data["toeic_listeningmaterial"]["material_id"] = material_id
#     for q in data["toeic_questions"]:
#         q["question_id"] = generate_uuid()
#         q["material_id"] = material_id

#     # å„²å­˜ JSONL
#     with open(JSONL_PATH, "a", encoding="utf-8") as f:
#         f.write(json.dumps(data, ensure_ascii=False) + "\n")

#     return {"data": data}

# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run("api_part2:app", host="0.0.0.0", port=11432, reload=True, log_level="debug")

from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
import chromadb
from sentence_transformers import SentenceTransformer
import random, json, re, uuid, os, traceback
from fastapi.middleware.cors import CORSMiddleware

# ===== App & CORS =====
app = FastAPI(title="TOEIC Part 2 Generator API (OpenAI)")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_methods=["*"], allow_headers=["*"]
)

# ===== Config =====
CHROMA_PATH = "/app/chroma_db"
JSONL_PATH  = "/app/json/toeic_part2_db_ready.jsonl"

OPENAI_MODEL    = os.getenv("OPENAI_MODEL", "gpt-4o-mini")   # å¯æ”¹æˆä½ è¦ç”¨çš„æ¨¡å‹
OPENAI_API_KEY  = os.getenv("OPENAI_API_KEY")                # å¿…å¡«

# ===== OpenAI Client =====
from openai import OpenAI
client_kwargs = {}
client = OpenAI(api_key=OPENAI_API_KEY, **client_kwargs)     # æ–°ç‰ˆ SDK ç”¨æ³•ï¼ˆResponses APIï¼‰  # :contentReference[oaicite:1]{index=1}

# ===== Embedding & Chroma =====
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
chroma_client   = chromadb.PersistentClient(path=CHROMA_PATH)
collection           = chroma_client.get_collection("toeic_part2")
scenario_collection  = chroma_client.get_collection("toeic_scenarios")

# ===== Schemaï¼ˆå¼·åˆ¶æ¨¡å‹è¼¸å‡ºæ ¼å¼ï¼‰=====
TOEIC_JSON_SCHEMA = {
    "type": "object",
    "additionalProperties": False,
    "required": ["toeic_listeningmaterial", "toeic_questions"],
    "properties": {
        "toeic_listeningmaterial": {
            "type": "object",
            "additionalProperties": False,
            "required": ["audio_url", "transcript", "accent", "topic", "speaker_count", "listening_level"],
            "properties": {
                "audio_url": {"type": ["string","null"]},
                "transcript": {"type": "string"},
                "accent": {"type": "string"},
                "topic": {"type": "string"},
                "speaker_count": {"type": "string"},
                "listening_level": {"enum": ["beginner","intermediate","advanced"]},
            }
        },
        "toeic_questions": {
            "type": "array",
            "minItems": 1, "maxItems": 1,
            "items": {
                "type": "object",
                "additionalProperties": False,
                "required": ["question_text","question_type","question_category","passage_id",
                             "question_image_url","part","option_a_text","option_b_text","option_c_text",
                             "is_correct","difficulty_level","explanation"],
                "properties": {
                    "question_text": {"type": "string"},
                    "question_type": {"type": "string", "enum": ["listen"]},
                    "question_category": {"type": "string", "enum": ["pos", "tense", "syntax", "vocab"]},
                    "passage_id": {"type": ["string","null"]},
                    "question_image_url": {"type": ["string","null"]},
                    "part": {"type": "string","enum": ["2"]},
                    "option_a_text": {"type": "string"},
                    "option_b_text": {"type": "string"},
                    "option_c_text": {"type": "string"},
                    "is_correct": {"enum": ["A","B","C"]},
                    "difficulty_level": {"enum":["1","2","3","4","5"]},
                    "explanation": {"type": "string"}
                }
            }
        }
    }
}

# ===== Utils =====
class GenerateRequest(BaseModel):
    query_text: str = "Part 2 TOEIC listening conversation with 3 option"

def clean_text(text: str) -> str:
    return re.sub(r'[\x00-\x09\x0b\x0c\x0e-\x1f\x7f]', '', text)

def generate_uuid() -> str:
    return str(uuid.uuid4())

@app.get("/healthz")
def healthz():
    # ç°¡æ˜“å¥åº·æª¢æŸ¥ï¼ˆä¸æ‰“ OpenAIï¼Œæª¢æŸ¥å¿…è¦è¨­å®šï¼‰
    return {
        "ok": True,
        "model": OPENAI_MODEL,
        "has_api_key": bool(OPENAI_API_KEY),
    }

# ===== æ ¸å¿ƒè·¯ç”± =====
@app.post("/generate_part2")
def generate_part2(req: GenerateRequest):
    try:
        # 1) éš¨æ©ŸæŒ‘æƒ…å¢ƒ
        scenario_keywords = ["ä¼æ¥­ç™¼å±•","å¤–é£Ÿ","å¨›æ¨‚","é‡‘èï¼é ç®—","ä¸€èˆ¬å•†å‹™","ä¿å¥",
                             "æˆ¿å±‹ï¼å…¬å¸åœ°ç”¢","è£½é€ æ¥­","è¾¦å…¬å®¤","äººäº‹","æ¡è³¼","æŠ€è¡“å±¤é¢","æ—…éŠ"]
        scenario_text = random.choice(scenario_keywords)

        # 2) æª¢ç´¢ç›¸ä¼¼é¡Œ + æƒ…å¢ƒç¤ºä¾‹ï¼ˆä¿æŒä½ çš„è¨­è¨ˆï¼‰
        query_embedding = embedding_model.encode(req.query_text).tolist()
        res_q = collection.query(query_embeddings=[query_embedding], n_results=3)
        similar_questions = res_q["documents"][0] if res_q["documents"] else []
        sample_ctx = f"Example: {similar_questions[0][:500]}..." if similar_questions else "Example: (no similar found)"

        scenario_embedding = embedding_model.encode(scenario_text).tolist()
        res_s = scenario_collection.query(query_embeddings=[scenario_embedding], n_results=1)
        scenario_context = res_s["documents"][0][0] if res_s["documents"] else scenario_text

        # 3) Promptï¼ˆäº¤çµ¦ OpenAI Responses APIï¼Œä¸¦ç”¨ JSON Schema é™åˆ¶è¼¸å‡ºï¼‰
        prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­ TOEIC å‡ºé¡Œå°ˆå®¶ã€‚æ ¹æ“šä¸‹åˆ—æ¸¬é©—æƒ…å¢ƒèˆ‡ç¯„ä¾‹é¡Œç›®ï¼Œç”Ÿæˆä¸€é¡Œæ–°çš„ TOEIC Part 2 è½åŠ›é¡Œçµ„ï¼ˆ**åªè¼¸å‡º JSONï¼Œç¬¦åˆæŒ‡å®š Schema**ï¼‰ï¼š

### æ¸¬é©—æƒ…å¢ƒï¼š
{scenario_context}

### ç¯„ä¾‹é¡Œç›®ï¼š
{sample_ctx}

è£½ä½œè¦é»ï¼š
- é¡Œç›®ç‚ºä¸€å¥å•å¥ï¼›å…¶å¾Œæœ‰ä¸‰å€‹é¸é …ï¼ˆA/B/Cï¼‰ï¼Œä¸¦åœ¨ transcript ä¸­å‘ˆç¾ã€‚
- å•†æ¥­è‹±æ–‡éœ€è‡ªç„¶ã€å°ˆæ¥­ï¼›ä¸‰å€‹é¸é …èªæ„éœ€æ¥è¿‘ä¸”åˆæ–‡æ³•ã€‚
- åªæœ‰explanation ç”¨ç¹é«”ä¸­æ–‡ï¼Œèªªæ˜æ–‡æ³•é‡é»æˆ–èªæ„å·®ç•°ï¼Œä¸å¯ç©ºæ³›ã€‚å…¶ä»–ç”¨è‹±æ–‡ã€‚
- **åš´æ ¼éµå®ˆ JSON Schema**ï¼Œä¸è¦æœ‰å¤šé¤˜æ¬„ä½
- topic è«‹ä½¿ç”¨ï¼š{scenario_text}
"""

         #  æ–°ç‰ˆ OpenAI SDK å‘¼å«
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role": "user", "content": prompt}],
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "toeic_part2",
                    "schema": TOEIC_JSON_SCHEMA,
                    "strict": True
                }
            }
        )

        raw_output = getattr(resp, "choices", [])[0].message.content if hasattr(resp, "choices") else None
        cleaned_output = clean_text(raw_output or "")

        try:
            data = json.loads(cleaned_output)
            parsing_success = True
        except Exception:
            # Fallback: OpenAI å¯èƒ½æ²’éµå®ˆ Schema â†’ è¿”å›åŸå§‹çµæœ
            data = {"raw_output": cleaned_output}
            parsing_success = False

       
# 6) å¦‚æœæˆåŠŸè§£æï¼Œè£œé½Š UUID
        if parsing_success and "toeic_listeningmaterial" in data:
            material_id = generate_uuid()
            data["toeic_listeningmaterial"]["material_id"] = material_id
            for q in data.get("toeic_questions", []):
                q["question_id"] = generate_uuid()
                q["material_id"] = material_id

            # è¿½åŠ å¯«å…¥ JSONLï¼ˆæŒä¹…åŒ–ï¼‰
            os.makedirs(os.path.dirname(JSONL_PATH), exist_ok=True)
            with open(JSONL_PATH, "a", encoding="utf-8") as f:
                f.write(json.dumps(data, ensure_ascii=False) + "\n")

        # 7) å›å‚³çµ¦ n8n
        return {
            "success": parsing_success,
            "data": data
        }

    except json.JSONDecodeError as e:
        raise HTTPException(status_code=502, detail=f"OpenAI å›å‚³é JSONï¼Œè§£æå¤±æ•—ï¼š{e}")
    except Exception as e:
        # å›å‚³è©³ç´°å †ç–Šï¼Œä¾¿æ–¼ä½ ç¾åœ¨æ’éŒ¯ï¼ˆä¸Šç·šå¯æ”¹ç‚ºéš±è—ï¼‰
        raise HTTPException(status_code=500, detail=traceback.format_exc())

if __name__ == "__main__":
    import uvicorn
    # å»ºè­°å›ºå®š 8000ï¼Œè®“ docker-compose ç”¨ 11432:8000 å°æ˜ 
    uvicorn.run("part2_api:app", host="0.0.0.0", port=8000, reload=True, log_level="debug")
